{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class WordEmbeddingManager:\n",
    "    \"\"\"\n",
    "    A manager class for word embeddings using numpy storage.\n",
    "    Includes common search operations and utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        with open(\".secrets.json\", \"r\") as f:\n",
    "            secrets = json.load(f)\n",
    "        self.client = OpenAI(api_key=secrets[\"openai_api_key\"])\n",
    "\n",
    "        self.embeddings: np.ndarray\n",
    "        self.words: List[str]\n",
    "        self.word_to_idx: Dict[str, int]\n",
    "\n",
    "    def load(self, filepath: str, n_rows: int | None = None):\n",
    "        \"\"\"\n",
    "        Load embeddings and metadata from CSV file.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(filepath, nrows=n_rows, encoding='utf-8')\n",
    "        self.embeddings = np.array([eval(emb) for emb in df[\"embedding\"]])\n",
    "        self.words = df[\"word\"].tolist()\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.words)}\n",
    "\n",
    "    def get_embedding(self, word: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the embedding for a specific word.\n",
    "        If word is not in vocabulary, generates a new embedding.\n",
    "        \"\"\"\n",
    "        if word not in self.word_to_idx:\n",
    "            return self.generate_embedding(word)\n",
    "        return self.embeddings[self.word_to_idx[word]]\n",
    "\n",
    "    def generate_embedding(self, word: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for a word.\n",
    "        \"\"\"\n",
    "        embedding = self.client.embeddings.create(input=[word], model=\"text-embedding-3-small\").data[0].embedding\n",
    "        return np.array(embedding)\n",
    "\n",
    "    def find_most_similar(\n",
    "        self,\n",
    "        query: Union[str, np.ndarray],\n",
    "        top_k: int = 5,\n",
    "        exclude_words: List[str] | None = None,\n",
    "        allow_input_words: bool = True,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find the most similar words to a query.\n",
    "        Query can be either a word or an embedding vector.\n",
    "\n",
    "        Args:\n",
    "            query: Input word or embedding vector\n",
    "            top_k: Number of similar words to return\n",
    "            exclude_words: List of words to exclude from the results\n",
    "            allow_input_words: If False, excludes words in exclude_words from results\n",
    "\n",
    "        Returns:\n",
    "            List of (word, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        if isinstance(query, str):\n",
    "            query_vector = self.get_embedding(query)\n",
    "        else:\n",
    "            query_vector = query\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = np.dot(self.embeddings, query_vector)\n",
    "\n",
    "        # Create a mask for excluded words if needed\n",
    "        if exclude_words and not allow_input_words:\n",
    "            exclude_indices = [self.word_to_idx[word] for word in exclude_words if word in self.word_to_idx]\n",
    "            similarities[exclude_indices] = -float(\"inf\")  # Set similarity to negative infinity for excluded words\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        return [(self.words[idx], similarities[idx]) for idx in top_indices]\n",
    "\n",
    "    def find_analogies(self, word1: str, word2: str, word3: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find word analogies (e.g., king - man + woman = queen).\n",
    "        \"\"\"\n",
    "        # Get embeddings for the words\n",
    "        emb1 = self.get_embedding(word1)\n",
    "        emb2 = self.get_embedding(word2)\n",
    "        emb3 = self.get_embedding(word3)\n",
    "\n",
    "        # Calculate target vector\n",
    "        target = emb1 - emb2 + emb3\n",
    "\n",
    "        return self.find_most_similar(target, top_k)\n",
    "\n",
    "    def find_between(self, word1: str, word2: str, ratio: float = 0.5, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find words that lie between two given words in the embedding space.\n",
    "        ratio determines how close to word1 vs word2 (0.5 = halfway between)\n",
    "        \"\"\"\n",
    "        emb1 = self.get_embedding(word1)\n",
    "        emb2 = self.get_embedding(word2)\n",
    "\n",
    "        # Calculate intermediate point\n",
    "        target = (1 - ratio) * emb1 + ratio * emb2\n",
    "\n",
    "        return self.find_most_similar(target, top_k)\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of words in the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.words) if self.words is not None else 0\n",
    "    \n",
    "    def cosine_similarity(self, query1: Union[str, np.ndarray], query2: Union[str, np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two queries (words or embedding vectors).\n",
    "        \n",
    "        Args:\n",
    "            query1: First input (word or embedding vector)\n",
    "            query2: Second input (word or embedding vector)\n",
    "        \n",
    "        Returns:\n",
    "            float: Cosine similarity score between -1 and 1\n",
    "        \"\"\"\n",
    "        # Convert words to embeddings if necessary\n",
    "        if isinstance(query1, str):\n",
    "            query1 = self.get_embedding(query1)\n",
    "        if isinstance(query2, str):\n",
    "            query2 = self.get_embedding(query2)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(query1, query2) / (np.linalg.norm(query1) * np.linalg.norm(query2))\n",
    "        return float(similarity)\n",
    "\n",
    "\n",
    "def pretty_print_similarities(similarities: List[Tuple[str, float]]):\n",
    "    for word, score in similarities:\n",
    "        # Ensure proper encoding when printing\n",
    "        try:\n",
    "            print(f\"{word.encode('utf-8').decode('utf-8')}: {score:.3f}\")\n",
    "        except UnicodeEncodeError:\n",
    "            # Fallback if there's an encoding error\n",
    "            print(f\"[encoding error for word]: {score:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = WordEmbeddingManager()\n",
    "manager.load(\"data/processed/top10000german.csv\", n_rows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_words = [\"katze\", \"hund\", \"fuchs\", \"kuh\"]\n",
    "\n",
    "# Find a word in the vocabulary that is most similar to the average of the blue words\n",
    "# For this we need to average the embeddings of the blue words and then find the word most similar to this average vector\n",
    "start_time = time.time()\n",
    "blue_embeddings = np.array([manager.get_embedding(word) for word in blue_words])\n",
    "blue_avg = np.mean(blue_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hunde: 0.538\n",
      "kühe: 0.527\n",
      "pferd: 0.456\n",
      "maus: 0.447\n",
      "hundert: 0.442\n",
      "tieren: 0.431\n",
      "pferde: 0.429\n",
      "schäfer: 0.429\n",
      "löwen: 0.428\n",
      "töpfer: 0.422\n"
     ]
    }
   ],
   "source": [
    "most_similar = manager.find_most_similar(blue_avg, top_k=10, exclude_words=blue_words, allow_input_words=False)\n",
    "pretty_print_similarities(most_similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45467978115723456"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.cosine_similarity(blue_avg, \"tier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codenames-kRJZUMYG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
